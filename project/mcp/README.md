mcp是一个纯异步网络事件通知，高性能，高可靠，可扩展的网络服务器框架。通用性高，提供了热重启，过载保护，负载监控、带缓冲的socker收发、插件化开发等特性。

我们nosql平台目前使用了4200台机器，构建了17万个实例，服务于腾讯视频、腾讯新闻、广告、体育、游戏等众多内部业务。日访问量达到4万亿次，峰值每秒1.1亿次访问。我在腾讯的两年主要就是负责参与BG级别的nosql平台共建，设计并开发了代理层pipeline、hashtag、流量压缩等特性，cache层，代理层是基于内部一个高性能的网络通信框架开发的代理服务，并在其上实现了权限校验、数据校验、分片路由与聚合、热key缓存、读写分离，命令禁用，负载统计等特性。我也深度参与了代理服务的开发。实现了pipeline、hashtag特性、流量压缩等特性。
# pipeline实现难点
1. 如何保证有序？
2. 如何保证热升级，减低对业务的影响？
## 如何保证有序？
设计前期，我先调研了一些业内pipeline的实现方式，
twitter开源的twme和豌豆荚开源的codis的实现方式类似，在按序将client的请求转发至后端cache处理之后，将请求按序压入client应答队列，后续将收到的cache应答包赋值给对应请求的一个成员变量，通过事件机制触发应答队列的回写。
```cpp
//twitter开源的`twme`是一个高性能单线程的事件驱动代理服务，它会为每个client连接维护一个接收队列和发送队列，接收到client请求之后，将请求压入`接收队列`，转发后端处理之后，将请求压入发送队列，后续收到cache应答之后，通过事件机制触发回写逻辑，按接收队列顺序回复保证pipeline实现。
//豌豆荚开源的`codis`也是类似的逻辑，在创建client连接对象时，开启读写两个协程，读协程读取client请求，转发至后端cache处理并将请求消息加入应答队列，后续收到cache应答后，将应答设置为请求的一个成员，并通知写协程轮询应答队列，将应答就绪的请求回复给client，保证请求pipeline。
```
我们nosql平台为什么不支持pipeline呢，主要是代理服务在路由转发之后就会释放对应的请求结构，没有做请求与应答的映射，无法保证应答顺序，回写服务只是简单的将client应答缓冲区的数据依次回写。同时为了提升并发能力，代理转发服务采用了多进程的架构，进行请求的路由分发，不能保证请求会顺序到达cache，收到的应答大概率会乱序。
设计的核心就是建立请求与应答的映射关系，保证按序回复。参考codis以及twme，对框架进行pipeline适配。设计一个序列号管理器，以序列号标记请求顺序，每个连接维护一个序列号FIFO队列，收到cache应答之后，将序列号标记为应答就绪状态，回写逻辑检查FIFO队列，按序回包即可。
## 如何保证热升级，减低对业务的影响？
proxy原先设计思路是将连接数据以及路由策略保存在共享内存里，来支持服务热重启、保证连接数据不丢失。pipeline的序列号队列作为连接数据的一部分，按理也需要保存在共享内存里，这就涉及到共享内存的动态扩展。分析了框架源码之后发现共享内存并没有足够的预留空间来存放序列号队列，需要对共享内存进行扩展，但是框架中的共享内存是基于systemV的接口开发的，只支持对以创建的共享内存的调小，不支持扩大。经过调查分析之后，决定兼容posix的共享内存接口，这里需要设计共享内存管理策略（原先systemV接口，有内核数据结构直接管理，删除方便）、做到无损升级、接口适配。
