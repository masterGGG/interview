框架的优劣势、机制、优化点、对比
epoll et
共享内存
eventfd+SHMMQ
内存池


mcp是一个纯异步网络事件通知，高性能，高可靠，可扩展的网络服务器框架。通用性高，提供了热重启，过载保护，负载监控、带缓冲的socker收发、插件化开发等特性。

我们nosql平台目前使用了4200台机器，构建了17万个实例，服务于腾讯视频、腾讯新闻、广告、体育、游戏等众多内部业务。日访问量达到4万亿次，峰值每秒1.1亿次访问。我在腾讯的两年主要就是负责参与BG级别的nosql平台共建，设计并开发了代理层pipeline、hashtag、流量压缩等特性，cache层，代理层是基于内部一个高性能的网络通信框架开发的代理服务，并在其上实现了权限校验、数据校验、分片路由与聚合、热key缓存、读写分离，命令禁用，负载统计等特性。我也深度参与了代理服务的开发。实现了pipeline、hashtag特性、流量压缩等特性。
# pipeline实现难点
1. 如何保证有序？
2. 如何保证热升级，减低对业务的影响？
## 如何保证有序？
设计前期，我先调研了一些业内pipeline的实现方式，
twitter开源的twme和豌豆荚开源的codis的实现方式类似，在按序将client的请求转发至后端cache处理之后，将请求按序压入client应答队列，后续将收到的cache应答包赋值给对应请求的一个成员变量，通过事件机制触发应答队列的回写。
```cpp
//twitter开源的`twme`是一个高性能单线程的事件驱动代理服务，它会为每个client连接维护一个接收队列和发送队列，接收到client请求之后，将请求压入`接收队列`，转发后端处理之后，将请求压入发送队列，后续收到cache应答之后，通过事件机制触发回写逻辑，按接收队列顺序回复保证pipeline实现。
//豌豆荚开源的`codis`也是类似的逻辑，在创建client连接对象时，开启读写两个协程，读协程读取client请求，转发至后端cache处理并将请求消息加入应答队列，后续收到cache应答后，将应答设置为请求的一个成员，并通知写协程轮询应答队列，将应答就绪的请求回复给client，保证请求pipeline。
```
我们nosql平台为什么不支持pipeline呢，主要是代理服务在路由转发之后就会释放对应的请求结构，没有做请求与应答的映射，无法保证应答顺序，回写服务只是简单的将client应答缓冲区的数据依次回写。同时为了提升并发能力，代理转发服务采用了多进程的架构，进行请求的路由分发，不能保证请求会顺序到达cache，收到的应答大概率会乱序。
设计的核心就是建立请求与应答的映射关系，保证按序回复。参考codis以及twme，对框架进行pipeline适配。设计一个序列号管理器，以序列号标记请求顺序，每个连接维护一个序列号FIFO队列，收到cache应答之后，将序列号标记为应答就绪状态，回写逻辑检查FIFO队列，按序回包即可。
## 如何保证热升级，减低对业务的影响？
proxy原先设计思路是将连接数据以及路由策略保存在共享内存里，来支持服务热重启、保证连接数据不丢失。pipeline的序列号队列作为连接数据的一部分，按理也需要保存在共享内存里，这就涉及到共享内存的动态扩展。分析了框架源码之后发现共享内存并没有足够的预留空间来存放序列号队列，需要对共享内存进行扩展，但是框架中的共享内存是基于systemV的接口开发的，只支持对以创建的共享内存的调小，不支持扩大。经过调查分析之后，决定兼容posix的共享内存接口，这里需要设计共享内存管理策略（原先systemV接口，有内核数据结构直接管理，删除方便）、做到无损升级、接口适配。

# 数据块内存池设计
采用Node+Chunk的方式设计，其中Node与Chunk的比例是M：N的关系。
Chunk是一块块固定大小的内存块，Node负责管理自身申请的一系列Chunk内存块，并以链表的方式组织成一块大的内存块。
各自以链表的形式串联成空闲队列。
基于这种定长Chunk思想，可以组合成对象哈希表、队列、缓冲区、树等。
优势：持久化，热重启、进程间通信

# 高性能轻量级进程间通信
MQ使用共享内存与FIFO实现了一个高效的进程间消息管道。
特殊说明的一点是，之所以选择这样的方式是因为FIFO构建在文件系统之上，访问FIFO的时候需要访问文件系统inode，这种操作是比较耗时的。但比起共享内存，FIFO可以很方便的与EPOLL配合，因此MQ的数据区选择使用高性能的共享内存，而FIFO仅用来作为触发装置。这样，应用进程只需要像写异步网络通讯程序那样通过EPOLL来管理MQ即可。
此外，当一个MQ仅被一个进程读或仅被一个进程写时，在读或者写时可不加锁。当MQ被多进程共享时，MQ会根据配置自动加相应的互斥锁。该互斥锁同样通过高效的POSIX信号量实现。
> 1. 可以满足频繁的，且对性能要求较高的多进程实时通讯；
> 2. 在通讯过程中可保证消息的时序，进而实现了高可靠的进程通信；
> 3. 可以避免通讯数据的冗余复制，尤其在消息数据会被多次转发的情况下，效果尤为明显；
> 4. 本进程通讯方法的信道MQ可以被多进程共享




# 实现对比 ptmalloc，tcmalloc、jemalloc
[](https://cloud.tencent.com/developer/article/1173720)

# 为什么使用共享内存？
访问共享内存区域和访问进程独有的内存区域一样快，并不需要通过系统调用或者其它需要切入内核的过程来完成。同时它也避免了对数据的各种不必要的复制。共享内存块提供了在任意数量的进程之间进行高效双向通信的机制。每个使用者都可以读取写入数据，往往与其他通信机制，如信号量配合使用，来实现进程间的同步和通信。而它的局限性也在于此．即共享内存的诸进程必须共处同一个计算机系统．有物理内存可以共享才行。共享在共享大数据文件时有用，直接在相同进行内存的拷贝，速度快，效率高，需要考虑访问临界资源并发同步。
可做持久化、热重启

# 速率控制算法
网络限速模块设计：
采用时间片数组统计单位时间内的数据量，实时计算收发速率。
