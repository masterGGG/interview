[toc]


# 共享内存模式修改
共享内存创建方式由systemv的shm_get改为Posix的tmpfs。

# client线程间跳转

# 线程负载

# client连接绑定处理线程

> TCP服务端口（主线程）
> MCP2CCS MQ（各线程，主线程初始化阶段直接add进处理线程epoll）
> CCSWorker MQ（各线程，主线程初始化阶段直接add进处理线程epoll）
> UDP服务端口（主线程）
> AMDIN服务端口（主线程）
> CLientTCP连接

# 1 开发细节
## 1.1 原逻辑简析
![enter image description here](https://github.com/masterGGG/common/blob/master/mcp-%E5%8E%9F%E9%80%BB%E8%BE%91.jpg)
    
如图，多线程CCS中锁主要分为3类：
> 1. 共享内存通信MQ读写锁，涉及多个进程之间的读写，暂不优化。

> 2. EpollFlow事件处理锁，考虑为每个处理线程分配一个EpollFlow，处理线程处理各自EpollFlow上的事件，取消事件处理锁，可优化去锁。

> 3. 每个TCP连接事件收发锁SocketNodeLock，考虑将每个TCP连接按一定算法分发到处理线程的EpollFlow中，这样单个连接的事件收发固定在某个处理线程，可优化去锁。

所有的TCP连接都放到一个EpollFlow中，所有的线程通过锁竞争获取EpollFlow中的事件进行处理。由于每个连接的读事件和写事件可能并发处理，还需要锁竞争SocketNodeLock。

## 1.2 改动逻辑简析
![enter image description here](https://github.com/masterGGG/common/blob/master/mcp-更新逻辑.jpg)


上图为改动之后的结构，其中`Thread 0`为主线程,`EpollFlow[0]`与原结构中的`EpollFlow`对应（支持热重启），`Thread n`是处理线程。主要作出如下改动：
> 1. 主线程处理的事件集合改为：MCP2CCS的回传数据处理事件，TCP端口的监听事件，管理admin端口以及UDP端口上的事件（监听，请求，应答）。

> 2. 处理线程处理的事件集合：本线程轮询的所有连接上的请求事件，本线程`Worker MQ[N]`上的通知事件。

> 3. 创建处理线程MQ以及处理线程`EpollFlow[n]`，并将MQ的读fd加入处理线程的`EpollFlow[n]`中，用于触发通知事件。

> 4. 主线程中accept TCP连接之后，组成连接通知包，并根据连接fd计算（对处理线程数取模）处理线程的id，写入对应处理线程MQ。

> 5. 处理线程不再处理MCP2CCS事件，只处理自己`EpollFlow[n]`以及`Worker MQ[N]`上的事件。

## 1.3 性能对比
下面为单机上，get和set请求比例10:1，value长度为512，32个连接异步请求的延迟分布。约减少**200微秒～800微秒**的延迟。
![enter image description here](https://km.woa.com/gkm/api/img/cos-file-url?url=https%3A%2F%2Fkm-pro-1258638997.cos.ap-guangzhou.myqcloud.com%2Ffiles%2Fphotos%2Fpictures%2F202004%2F1587560207_80_w587_h190.png&is_redirect=1)
![enter image description here](https://km.woa.com/gkm/api/img/cos-file-url?url=https%3A%2F%2Fkm-pro-1258638997.cos.ap-guangzhou.myqcloud.com%2Ffiles%2Fphotos%2Fpictures%2F202004%2F1587560282_59_w586_h193.png&is_redirect=1)
![enter image description here](https://km.woa.com/gkm/api/img/cos-file-url?url=https%3A%2F%2Fkm-pro-1258638997.cos.ap-guangzhou.myqcloud.com%2Ffiles%2Fphotos%2Fpictures%2F202004%2F1587560306_96_w582_h191.png&is_redirect=1)

# 2 TODO
## 2.1 主线程写数据到处理线程MQ失败的处理方式？关闭连接？写数据到主线程的workerMQ重新分发？
主线程写处理线程MQ有三种数据包
1. TCP连接建立包
2. TCP数据应答包
3. TCP连接断开包
写入失败一般是由于MQ写满导致的，此时无法通知对应的处理线程处理事件,尝试写连接断开包至MQ，还是失败就直接关闭连接。

## 2.2 MQ改造
### 2.2.1 添加基于tmpfs的Posix方式的共享内存MQ（Done）
### 2.2.2 添加配置，增加基于文件名创建MQ（Done）
### 2.2.3 tmpfs文件删除逻辑?(Done)
文件一直留存？链接为零后删除文件？如何获取链接数，直接在内存头部里记录链接次数？
### 2.2.4 测试Posix和SystemV MQ的性能差异(Done)
### 2.2.5 Posix MQ热重启
posix和systemv的共享内存灰度时，是否需要数据备份，防止数据丢失？

## 2.3 固定连接

# 3 DONE
## 3.1 配置参数`UnlockedMode`？
    设定配置参数UnlockedMode，是否需要开启无锁模式。
## 3.2 每个线程一个`EpollFlow`？
    为每个处理线程分配EpollFlow，初始化，热重启时先删除EpollFlow，在初始化时为每个建立的连接重新分配处理线程。
### 3.2.1 `CEpollFlow`改为无锁模式
* > 无锁化(done)

    监听端口有新连接时，主线程accept接收FD，同时创建连接fd的数据包通知对应的worker线程有fd需要加入CEpollFlow。（DONE）
## 3.3 连接分配原则？
    连接分配原则，连接Fd直接对m_threadnum取模。
    
## 3.4 不同线程通信？
* > 1. 为每个线程创建MQ，参考CCS与MCP通信的MQ逻辑。
* > 2. 主线程监听处理每个MCP到CCS的MQ FD。

CCS&MCP共有几种数据格式？各怎么处理？
传输数据格式：
```cpp
 CMD_DATA_TRANS = 0,        	//通用数据传输包（TCP-TCP_CLIENT_SOCKET & UDP-UDP_SOCKET），TCP数据需要分发至处理线程进行回包
 CMD_CCS_NOTIFY_DISCONN,     	//通知断开连接，只针对TCP连接,暂不需要
 CMD_NOTIFY_STAT,		//只有主线程初始化和状态回写文件时使用，处理线程不需要考虑
 CMD_CUSTOM_MCP_MSG		//MCP回传给CCS的数据包，在主线程处理
```

每个线程需要处理的FD类型
|TYPE|main|worker1|worker2|worker3|FUNC|
|---|---|---|---|---|---|
|TCP_CLIENT_SOCKET||V|V|V|RecvTCP|
|MQ_PIPE_SOCKET|V||||ProcessMQ|
|WORKER_MQ_PIPE_SOCKET||V|V|V|ProcessMQ|
|TCP_LISTEN_SOCKET|V||||ProcessAccept|
|ADMIN_LISTEN_SOCKET|V||||ProcessAccept|
|ADMIN_CLIENT_SOCKET|V||||ProcessAdminMsg|
|UDP_SOCKET|V||||RecvUDP|

## 3.5 线程处理属于自己的连接FD数据？
    在主线程中处理MCP到CCS队列的数据包时，先根据数据包头部检测，将应答包拷贝至连接所在的处理线程的WORKER_MQ中。

## 3.6 UDP请求应答如何处理
    UDP监听FD直接在主线程中处理，由于MCP->CCS的队列也是在主线程中处理，队列中的UDP应答包也会在主线程中处理。

## 3.7 升级热重启失败？
    在接收到热重启信号后，释放工作线程EpollFlow，否则会造成fd无效占用。热重启时，需要重新为每个工作线程分配EpollFlow和WorkerMQ_fd，需要先清除重启前WorkerMQ_fd占用的SocketNode，否则CreateScoketNode会失败（因为SocketNode已经被占用）。

## 3.8 降级热重启？
    无锁版本工作线程处理各自的EpollFlow中的连接fd，在热重启时，将工作线程处理的连接fd加入到主线程的EpollFlow，即可保证连接不丢失。

