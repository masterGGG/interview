
# 介绍一下你的项目
我参与的istore系统是PCG通用KV存储平台，集高性能分布式存储、自动化运维、质量报表、失败监控告警等于一体，具有热搬迁，异地融灾等高可用特性。广泛应用于视频、新闻、体育、广告等业务部门。
我们这个存储平台采用了分层设计，可以分为管理层、HA层、代理层、存储层。
1. 管理层是存储平台的web端，展示质量视图（包括机器级别，服务级别，集群级别，以及平台级别的数据分析视图）；支持服务搬迁、扩缩容；
2. HA层是负责对集群中的代理集群以及存储集群心跳探测，代理层路由下发以及存储层的故障切换。
3. 代理层提供读写分离、连接收敛、过载保护、请求数据统计等功能。
4. 存储层是一个抽象，底层是不同的存储服务，比如redis，bdb，ssd，ranklist等。
我们存储平台适用于高性能、耗时敏感的在线服务；数据结构简单，不需要强一致性的业务。

# 你遇到的难点？是如何解决的？
我就讲讲我做的pipeline特性吧。我们自研的存储服务，不支持pipeline功能。我们的代理层是基于PCG平台自研、高性能，高可靠，可扩展的网络框架开发的。
这个框架分为3个独立的模块，包括前端交互模块、逻辑处理模块和后端转发模块，模块间通过共享内存点对点通信。
1. 前端交互模块是一个多线程服务，负责响应客户端请求并转发消息、回写应答给客户端。
2. 逻辑处理模块是一个多线程服务，负责消息的逻辑处理，包括数据校验，鉴权，统计等功能。
3. 后端转发模块是将处理好的请求转发至存储层。
消息传递的过程中，为了提高吞吐、负载均衡，针对读请求采用轮询的方式分发给逻辑处理模块处理，不能保证收到的响应数据与请求数据一致。

设计前期，我先调研了一些业内pipeline的实现方式，
twme和codis的实现方式类似，两者的核心思想是为每个客户端连接维护一个请求队列，应答数据作为请求结构的一个成员，按请求顺序回复客户端。
twme无法平滑扩容，没有管理界面。codis是一套成熟的redis分布式系统，依赖组件多，迁移成本大。最后决定在已有框架上进行pipeline特性的支持。这里的问题是前端交互模块不缓存请求消息，无法维护请求队列，需要为连接维护一个请求队列。设计一个序列号管理器，以序列号标记请求顺序，每个连接维护一个序列号FIFO队列，收到cache应答之后，将序列号标记为应答就绪状态，回写逻辑检查FIFO队列，按序回包即可。

proxy原先设计思路是将连接数据以及路由策略保存在共享内存里，来支持服务热重启、保证连接数据不丢失。pipeline的序列号队列作为连接数据的一部分，也需要保存在共享内存里，这就涉及到共享内存的动态扩展。分析了框架源码之后发现共享内存并没有足够的预留空间来存放序列号队列，需要对共享内存进行扩展，但是框架中的共享内存是基于systemV的接口开发的，只支持对以创建的共享内存的调小，不支持扩大。经过调查分析之后，决定兼容posix的共享内存接口，这里需要设计共享内存管理策略（原先systemV接口，有内核数据结构直接管理，删除方便）、做到无损升级、接口适配。

# 路由策略 有限一致性hash
jump consist hash：