
# 介绍一下你的项目
我参与的istore系统是PCG通用KV存储平台，为PCG大部分业务员以及其他BG部分业务提供分布式存储服务。
这个存储平台采用了分层架构设计，可以分为应用层、代理层、存储层、HA层和管理平台5个部分。
1. 应用层部署在业务侧，负责从名字服务获取代理节点并进行交互。
2. 代理层用于客户端请求的接收与转发，提供连接收敛、过载保护以及运营质量统计，运营质量包括QPS，延时，流量等运营指标，为管理平台运营指标提供元数据。
3. 存储层包含一系列存储服务，比如redis，ssdb（使用LevelDB作为存储引擎, 支持T级别数据存储的nosql数据库），ranklist（是一个基于redis改造，为推荐召回场景提供的分布式索引存储计算系统）等。
4. HA层属于系统监控者，负责监控集群状态，对代理节点进行路由下发以及异常剔除，对存储节点的故障切换。
5. 管理平台提供运维工具和运营数据。其中运维工具包括业务自定义配置、一键部署；节点升级；服务搬迁、扩缩容等；运营数据包括机器视图、业务视图和平台视图。机器视图包括机器内存适用、cpu、连接数、出入流量等基础指标；业务视图包括请求量、延时、失败率等业务指标；平台视图包括日请求量、存储总容量、空闲资源统计等平台指标。

# 你遇到的难点？是如何解决的？
在开发pipeline特性时，由于proxy层采用的自研网络框架本身历史设计，导致istore平台不支持pipeline特性。这里遇到的第一个难点是istore多个组件依赖于该网络框架，必须保证设计的通用性和独立性。第二个难点是proxy节点可以服务于多个业务，必须确保服务无损升级，特性业务隔离，互不影响。
网络框架可以分为server层、worker层和client层3个部分。
1. server层是一个单reactor多线程服务，负责监听TCP/UDP端口，接收外部的请求，并通过共享内存队列通知worker模块处理。
2. worker层是一个多线程服务，负责业务请求的逻辑处理，将请求以及路由策略下发给client进行转发。
3. client层是一个单reactor单线程服务，通过连接池机制管理后台服务连接。
网络框架各层通过共享内存MQ点对点通信。请求流程是server层接收请求包，添加server头之后，压入至共享内存MQ。worker层从共享内存MQ中获取请求包，进行逻辑处理，构建请求对象，缓存server头，构造worker头和路由信息，压入共享内存MQ。client层从MQ中拿到请求包，根据worker头中的路由信息转发请求给后端服务处理。
应答逻辑是client层收到后端服务的应答包，直接压入对应的MQ。worker层收到对应MQ中的应答包，触发请求对象进行逻辑处理，将缓存的server头和最终应答包压入MQ。server层消费MQ中的应答包，根据server头中的连接信息，将应答包回复给客户端。
框架改造的困难在于如何





在开发pipeline特性时，由于代理节点的服务器开发框架本身设计原因，致使istore平台不支持pipeline特性，要支持pipeline特性，就需要对服务器框架本身进行改造。这里遇到的第一个难点是istore多个组件依赖于该网络框架，必须保证设计的通用性和独立性。第二个难点是代理节点可以服务于多个业务，必须确保服务无损升级，特性业务隔离，互不影响。还有一个难点是服务器框架采用多进程开发，请求转发无状态，如何维护请求有序？
这个网络框架是腾讯一款通用服务器开发框架，提供高性能网络交互以及Cache，日志，队列等通用组件。该框架分为前端收发、业务处理和后端收发3个模块：
1. 前端收发模块是一个单reactor多线程服务，负责监听TCP/UDP端口，接收外部的请求，并将业务处理模块的回复包发送出去。
2. 业务处理模块是一个多线程服务，是连接前端收发模块与后端收发模块的桥梁，负责业务逻辑处理，支持so的方式加载业务逻辑，将处理后的数据转发至前端或者后端。
3. 后端收发模块是一个单reactor单线程服务，负责对外连接，借助外部网络服务进一步处理业务请求。
模块间通过共享内存点对点通信。请求转发过程中是无状态的，不能保证收到的响应数据与请求顺序一致。对于批量请求的业务，需要业务方对请求结果排序，编码逻辑复杂化，性能开销大。

设计前期，我先调研了一些pipeline的业内实现，twme_proxy和codis的实现类似，两者的核心思想是为每个客户端维护一个请求队列，应答数据作为请求结构的一个成员，按请求顺序回复客户端。
twme是单线程模型，无法平滑扩容，没有管理界面。
codis是一套成熟的redis分布式系统，依赖组件多，无法单独替换。
我们存储平台在代理层做了很多定制化设计，迁移成本大，最后决定在当前通用框架上支持pipeline。

设计的核心是客户端维护一个请求队列。由于请求转发过程中是无状态的，我就想着为每个请求分配一个序列号，设计一个序列号管理器，以序列号标记请求顺序，每个连接维护一个序列号FIFO队列，收到cache应答之后，将序列号标记为应答就绪状态，回写逻辑检查FIFO队列，按序回包即可。

proxy原先设计思路是将连接数据以及路由策略保存在共享内存里，来支持服务热重启、保证连接数据不丢失。pipeline的序列号队列作为连接数据的一部分，也需要保存在共享内存里，这就涉及到共享内存的动态扩展。分析了框架源码之后发现共享内存并没有足够的预留空间来存放序列号队列，需要对共享内存进行扩展，但是框架中的共享内存是基于systemV的接口开发的，只支持对以创建的共享内存的调小，不支持扩大。经过调查分析之后，决定兼容posix的共享内存接口，这里需要设计共享内存管理策略（原先systemV接口，有内核数据结构直接管理，删除方便）、做到无损升级、接口适配。

# 路由策略 有限一致性hash
jump consist hash：